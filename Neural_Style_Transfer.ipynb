{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YnxySjRaQZjh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969,
     "referenced_widgets": [
      "c50fc077efa745f6a6ee96d26f16e3b5",
      "c05b84efd5334c20af4509ad5f05888c",
      "fa2be2c84c904ea7b899e1bf187bb3c8",
      "95f1dec098f243c891cd2b31a7c4f95a",
      "ed2c403a6e8046ac87dc4f9fbec0300e",
      "d4946316044040ce82ce09f9187d8854",
      "b69c29bb60324d43ba28fc1d90fcfd64",
      "30d5f76fb26a4946856eac71a1a0d80c"
     ]
    },
    "colab_type": "code",
    "id": "lgLFGp74Un8e",
    "outputId": "c537fd2f-4bc1-4be2-bb81-fc0fa8c67d54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50fc077efa745f6a6ee96d26f16e3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19 = models.vgg19(pretrained=True)\n",
    "vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa0wY9n_rxct"
   },
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7itASWda4T_"
   },
   "outputs": [],
   "source": [
    "class NST(nn.Module):\n",
    "  def __init__(self, max_pool=False):\n",
    "    super(NST, self).__init__()\n",
    "    vgg19 = models.vgg19(pretrained=True)\n",
    "    # self.vgg19features = nn.ModuleList(list(vgg19.features))\n",
    "    self.vgg19features = vgg19.features\n",
    "    for i in range(len(self.vgg19features)):\n",
    "      if isinstance(self.vgg19features[i], nn.ReLU):\n",
    "        self.vgg19features[i] = nn.ReLU(inplace=False)\n",
    "      if not max_pool:\n",
    "        if isinstance(self.vgg19features[i], nn.MaxPool2d):\n",
    "          self.vgg19features[i] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    # self.vgg19features = nn.Sequential(self.vgg19features)\n",
    "    print(self.vgg19features)\n",
    "  \n",
    "  def forward(self, image, content_layer=21, style_layers=[1,6,11,20,29], content_reconstruction=False, style_reconstruction=False):\n",
    "    if content_reconstruction:\n",
    "      for i, layer in enumerate(self.vgg19features):\n",
    "        if i == 0:\n",
    "          content_out = layer(image)\n",
    "          # x = layer(content_image)\n",
    "          # y = layer(white_noise_image)\n",
    "        else:\n",
    "          content_out = layer(content_out)\n",
    "          # x = layer(x)\n",
    "          # y = layer(y)\n",
    "          if i == content_layer:\n",
    "            content_image_representation = content_out\n",
    "            # content_image_representation = x\n",
    "            # white_noise_image_representation = y\n",
    "    \n",
    "    if style_reconstruction:\n",
    "      style_outs = []\n",
    "      # style_layer_outs = []\n",
    "      # white_layer_outs = []\n",
    "      for i, layer in enumerate(self.vgg19features):\n",
    "        if i == 0:\n",
    "          style_out = layer(image)\n",
    "          # x = layer(style_image)\n",
    "          # y = layer(white_noise_image)\n",
    "        else:\n",
    "          style_out = layer(style_out)\n",
    "          # x = layer(x)\n",
    "          # y = layer(y)\n",
    "        if i in style_layers:\n",
    "          style_outs.append(style_out[0])\n",
    "          # style_layer_outs.append(x[0])\n",
    "          # white_layer_outs.append(y[0])\n",
    "    \n",
    "    # return content_image_representation[0], white_noise_image_representation[0], style_layer_outs, white_layer_outs\n",
    "    if content_reconstruction:\n",
    "      return content_image_representation[0]\n",
    "    \n",
    "    if style_reconstruction:\n",
    "      return style_outs\n",
    "  \n",
    "  def inference(self, content_image, style_image, white_noise_image, device, alpha=1, beta=1e2, content_layer=21, style_layers=[1,6,11,20,29], epochs=10, required_loss=0.01, content_reconstruction=True, style_reconstruction=True):\n",
    "    print(\"before normalize: \")\n",
    "    # d1, d2, d3 = content_image.shape\n",
    "    plt.imshow(content_image); plt.show()\n",
    "    plt.imshow(style_image); plt.show()\n",
    "    plt.imshow(white_noise_image); plt.show()\n",
    "\n",
    "    # content_image_norm = self.normalize(content_image)\n",
    "    # style_image_norm = self.normalize(style_image)\n",
    "    # mean, var = 0, 0.1\n",
    "    # white_noise_image = content_image + np.random.normal(mean, var**0.5, content_image.shape)\n",
    "    # white_noise_image = np.random.normal(mean, var**0.5, content_image.shape)\n",
    "    # white_noise_image = deepcopy(content_image)\n",
    "    # plt.imshow(white_noise_image.reshape((d2,d3,d1))); plt.show()\n",
    "    # white_noise_image_norm = self.normalize(white_noise_image)\n",
    "\n",
    "    content_image_tensor = self.preprocess(content_image).unsqueeze(0).to(device)\n",
    "    style_image_tensor = self.preprocess(style_image).unsqueeze(0).to(device)\n",
    "    white_noise_image_tensor = self.preprocess(white_noise_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # content_image_tensor = torch.from_numpy(content_image_norm).float().to(device).unsqueeze_(0)\n",
    "    # style_image_tensor = torch.from_numpy(style_image_norm).float().to(device).unsqueeze_(0)\n",
    "    # white_noise_image_tensor = torch.tensor(white_noise_image_norm).float().to(device).unsqueeze_(0)\n",
    "\n",
    "\n",
    "    # print(\"after normalize, denormalize: \")\n",
    "    # plt.imshow(self.denormalize(content_image_tensor[0]).numpy().reshape((224,224,3))); plt.show()\n",
    "    # plt.imshow(self.denormalize(style_image_tensor[0]).numpy().reshape((224,224,3))); plt.show()\n",
    "    # plt.imshow(self.denormalize(white_noise_image_tensor[0]).numpy().reshape((224,224,3))); plt.show()\n",
    "\n",
    "\n",
    "    white_noise_image_tensor = white_noise_image_tensor.requires_grad_()\n",
    "    # print(content_image_tensor.unsqueeze(0).shape, style_image_tensor.unsqueeze(0).shape, white_noise_image_tensor.unsqueeze(0).shape)\n",
    "    \n",
    "    # optimizer = optim.SGD([white_noise_image_tensor], lr=0.001, momentum=0.9)\n",
    "    # optimizer = optim.Adam([white_noise_image_tensor], lr=0.001)\n",
    "    optimizer = optim.LBFGS([white_noise_image_tensor])\n",
    "\n",
    "    # print(\"before loop:\")\n",
    "    # white_noise_image_np = torch.tensor(white_noise_image_tensor).numpy()[0]\n",
    "    # plt.imshow(white_noise_image_np.reshape(224,224,3)); plt.show();\n",
    "\n",
    "    # content_image_tensor.unsqueeze_(0)\n",
    "    # style_image_tensor.unsqueeze_(0)\n",
    "    # white_noise_image_tensor.unsqueeze_(0)\n",
    "\n",
    "    # loss = float('inf')\n",
    "    count = 0\n",
    "    # while loss > required_loss and count<epochs:\n",
    "    content_image_representation = self(content_image_tensor, content_layer=content_layer, content_reconstruction=True)\n",
    "    style_layer_outs = self(style_image_tensor, style_layers=style_layers, style_reconstruction=True)\n",
    "    while count < epochs:\n",
    "      print(\"EPOCH NO: \", count+1)\n",
    "      def closure():\n",
    "        # white_noise_image_tensor.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        white_noise_image_representation = self(white_noise_image_tensor, content_layer=content_layer, content_reconstruction=True)\n",
    "        white_layer_outs = self(white_noise_image_tensor, style_layers=style_layers, style_reconstruction=True)\n",
    "        # print(content_image_representation.shape, white_noise_image_representation.shape, style_layer_outs[1].shape, white_layer_outs[3].shape)\n",
    "        # loss = self.totalLoss(content_image_representation, white_noise_image_representation, style_layer_outs, white_layer_outs, alpha=alpha, beta=beta)\n",
    "        weight = 1/len(style_layer_outs)\n",
    "        content_loss = self.contentLoss(content_image_representation, white_noise_image_representation)\n",
    "        style_loss = sum([weight*self.styleLoss(style_layer_outs[i], white_layer_outs[i]) for i in range(len(style_layer_outs))])\n",
    "        total_loss = alpha*content_loss + beta*style_loss\n",
    "        print(\"total loss: \", total_loss)\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        return total_loss\n",
    "\n",
    "      optimizer.step(closure)\n",
    "\n",
    "      # print(\"after step: \")\n",
    "      # print(white_noise_image_tensor)\n",
    "      # print(\"LOSS: \",loss)\n",
    "      # white_noise_image_np = self.denormalize(torch.tensor(white_noise_image_tensor[0])).cpu().numpy()\n",
    "      white_noise_image_pil = self.postprocess(torch.tensor(white_noise_image_tensor[0]).cpu())\n",
    "      # content_image_np = self.denormalize(torch.tensor(content_image_tensor[0])).cpu().numpy()\n",
    "      # style_image_np = self.denormalize(torch.tensor(style_image_tensor[0])).cpu().numpy()\n",
    "      plt.imshow(white_noise_image_pil); plt.show();\n",
    "      # plt.imshow(content_image_np.reshape((d2,d3,d1))); plt.show();\n",
    "      # plt.imshow(style_image_np.reshape((d2,d3,d1))); plt.show();\n",
    "      count += 1\n",
    "    \n",
    "    # white_noise_image_tensor.squeeze_(0)\n",
    "    # white_noise_image_tensor_denorm = self.denormalize(white_noise_image_tensor.detach().cpu()[0])\n",
    "    \n",
    "    # return white_noise_image_tensor_denorm.numpy()\n",
    "    return self.postprocess(white_noise_image_tensor.squeeze(0).detach().cpu())\n",
    "  \n",
    "  def preprocess(self, image):\n",
    "    img_size=256\n",
    "    pre = transforms.Compose([transforms.Scale(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "    return pre(image)\n",
    "  \n",
    "  def postprocess(self, image):\n",
    "    post1 = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "    post2 = transforms.Compose([transforms.ToPILImage()])\n",
    "    tnsr = post1(image)\n",
    "    tnsr[tnsr>1] = 1\n",
    "    tnsr[tnsr<0] = 0\n",
    "    return post2(tnsr)\n",
    "\n",
    "  def contentLoss(self, content_image_representation, white_noise_image_representation):\n",
    "    return nn.MSELoss()(content_image_representation, white_noise_image_representation)\n",
    "  \n",
    "  def styleLoss(self, style_layer_outs, white_layer_outs):\n",
    "    def gramMatrix(x):\n",
    "      d1, d2, d3 = x.shape\n",
    "      x_reshape = x.view(d1, d2*d3)\n",
    "      return torch.mm(x_reshape, x_reshape.t()).div(d1*d2*d3)\n",
    "    \n",
    "    style_gram = gramMatrix(style_layer_outs)\n",
    "    white_gram = gramMatrix(white_layer_outs)\n",
    "\n",
    "    return nn.MSELoss()(style_gram, white_gram)\n",
    "  \n",
    "  def normalize(self, image):\n",
    "    # img = torch.tensor(img).float()\n",
    "    img = image/255.\n",
    "    # mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    # std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    img[0, :, :] -= mean[0]\n",
    "    img[1, :, :] -= mean[1]\n",
    "    img[2, :, :] -= mean[2]\n",
    "\n",
    "    #divide by std\n",
    "    img[0, :, :] /= std[0]\n",
    "    img[1, :, :] /= std[1]\n",
    "    img[2, :, :] /= std[2]\n",
    "  \n",
    "    return img\n",
    "  \n",
    "  def denormalize(self, image):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "    img = deepcopy(image)\n",
    "\n",
    "    img[0, :, :] *= std[0]\n",
    "    img[1, :, :] *= std[1]\n",
    "    img[2, :, :] *= std[2]\n",
    "\n",
    "    img[0, :, :] += mean[0]\n",
    "    img[1, :, :] += mean[1]\n",
    "    img[2, :, :] += mean[2]\n",
    "    \n",
    "    img*=255.\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzsQAw2-xK2N"
   },
   "outputs": [],
   "source": [
    "def imageLoad(path, size=256):\n",
    "  # path=os.getcwd()+\"/img1.jpg\"\n",
    "  # print(path)\n",
    "  # img=cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "  # img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = Image.open(path)\n",
    "  img = img.resize((size, size))\n",
    "  # img = np.asarray(img)/1.\n",
    "  # print(img.shape)\n",
    "  # img = cv2.resize(img, (500, 500)).reshape((3, 500, 500))\n",
    "  # img = np.float64(img)/255.\n",
    "  return img\n",
    "\n",
    "def imageSave(img, path, fmt='PNG'):\n",
    "  # img1=cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "  # cv2.imwrite(path, img)\n",
    "  img.save(path, format=fmt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ICq6hK4xyEw"
   },
   "outputs": [],
   "source": [
    "path_to_sml = \"/content/drive/My Drive/SML Project\"\n",
    "path_to_content_images = os.path.join(path_to_sml, \"Content images\")\n",
    "path_to_style_images = os.path.join(path_to_sml, \"Style images\")\n",
    "path_to_nst_images = os.path.join(path_to_sml, \"NST images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "3nn6cCnPuvFD",
    "outputId": "854c3fc4-70b2-4b65-f210-086ad40bbffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU()\n",
      "  (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU()\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU()\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU()\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU()\n",
      "  (18): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU()\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU()\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU()\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU()\n",
      "  (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU()\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU()\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU()\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU()\n",
      "  (36): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nst = NST().to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5wSbWtlvGS2"
   },
   "outputs": [],
   "source": [
    "content_image1 = imageLoad(os.path.join(path_to_content_images, \"content3.jpeg\"))\n",
    "# style_image1 = imageLoad(os.path.join(path_to_style_images, \"picasso.jpg\"))\n",
    "style_image1 = imageLoad(os.path.join(path_to_style_images, \"picasso.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxW2sVnm5ZiT"
   },
   "outputs": [],
   "source": [
    "# print(np.float64(content_image1).dtype)\n",
    "# plt.imshow(content_image1.reshape((500,500,3)))\n",
    "# plt.show()\n",
    "# plt.imshow(style_image1.reshape((500,500,3)))\n",
    "# plt.show()\n",
    "\n",
    "plt.imshow(content_image1); plt.show()\n",
    "plt.imshow(style_image1); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkWpZUz5ySeY"
   },
   "outputs": [],
   "source": [
    "input_image = deepcopy(content_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyXVTsLlyy83"
   },
   "outputs": [],
   "source": [
    "alpha = 1e1\n",
    "beta = 1e7\n",
    "content_layer = 21\n",
    "# style_layers = [1,6,11,20,29]\n",
    "style_layers = [1,6,11,20,29]\n",
    "epochs = 100\n",
    "start_time = time.time()\n",
    "nst_image = nst.inference(content_image1, style_image1, input_image, device, epochs=epochs, alpha=alpha, beta=beta)\n",
    "end_time = time.time()\n",
    "print(\"time taken: \", (end_time - start_time), (end_time - start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4ONMtttCLxT"
   },
   "outputs": [],
   "source": [
    "plt.imshow(nst_image);plt.show()\n",
    "plt.imshow(content_image1);plt.show()\n",
    "plt.imshow(style_image1); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3Pgo8igCIwe"
   },
   "outputs": [],
   "source": [
    "imageSave(nst_image, os.path.join(path_to_nst_images, \"nst_image5\"+str((alpha, beta, content_layer, style_layers,epochs))+\".jpg\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SML Neural Style Transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "30d5f76fb26a4946856eac71a1a0d80c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95f1dec098f243c891cd2b31a7c4f95a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30d5f76fb26a4946856eac71a1a0d80c",
      "placeholder": "​",
      "style": "IPY_MODEL_b69c29bb60324d43ba28fc1d90fcfd64",
      "value": " 548M/548M [11:31&lt;00:00, 831kB/s]"
     }
    },
    "b69c29bb60324d43ba28fc1d90fcfd64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c05b84efd5334c20af4509ad5f05888c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c50fc077efa745f6a6ee96d26f16e3b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2be2c84c904ea7b899e1bf187bb3c8",
       "IPY_MODEL_95f1dec098f243c891cd2b31a7c4f95a"
      ],
      "layout": "IPY_MODEL_c05b84efd5334c20af4509ad5f05888c"
     }
    },
    "d4946316044040ce82ce09f9187d8854": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed2c403a6e8046ac87dc4f9fbec0300e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fa2be2c84c904ea7b899e1bf187bb3c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4946316044040ce82ce09f9187d8854",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed2c403a6e8046ac87dc4f9fbec0300e",
      "value": 574673361
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
